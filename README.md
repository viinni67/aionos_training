**ğŸš€ RAG-Powered Chatbot with Streamlit + FastAPI (Groq API + GPT-OSS-120B)**
ğŸ“Œ **Overview**
This project is a Retrieval-Augmented Generation (RAG) based chatbot with:

Frontend: Built with Streamlit for an interactive and lightweight chat interface.

Backend: Powered by FastAPI for handling requests, orchestrating responses, and managing retrieval logic.

LLM Engine: Uses the Groq API with the model openai/gpt-oss-120b for high-quality, low-latency responses.

Vector Database: ChromaDB for context retrieval, pre-trained on Google Colab and loaded into the backend for fast querying.

This architecture enables context-aware, domain-specific responses while maintaining guardrails for safety and relevance.

[ User ]  
   â¬‡ (Query)  
[ Streamlit Frontend ] â€”(API Call)â†’ [ FastAPI Backend ]  
   â¬‡                                   â¬‡  
[ Chroma Vector DB ] â†â€” Retriever â€” [ Groq API (GPT-OSS-120B) ]  


**Prompt Engineering & Guardrails**

We implemented prompt engineering to ensure the chatbot remains safe, accurate, and on-topic.

Approach:

Context Injection â€“ The retrieved knowledge chunks from Chroma are inserted into the prompt as context.

Instruction Layer â€“ A fixed system prompt ensures the model answers only from provided context.

Guardrails â€“ Added conditional logic to handle:

Irrelevant Queries â€“ Respond politely when context doesnâ€™t match user query.

Harmful Queries â€“ Refuse unsafe or unethical requests.

Fallback Behavior â€“ If no relevant context is found, the model responds with a helpful fallback message rather than hallucinating.

Example System Prompt:






ğŸ—‚** Vector Database with Chroma**

We use Chroma as our vector store for efficient semantic search.

Workflow:

Data Preparation â€“ Documents are cleaned and chunked in Google Colab.

Embedding Generation â€“ Pre-trained embeddings are created and stored in Chroma.

Persistence â€“ The Chroma DB is saved locally after training.

Loading into Backend â€“ On FastAPI startup, the persisted Chroma DB is loaded into memory for retrieval.

This allows the chatbot to instantly fetch top-K relevant chunks for each query.





**Installation & Setup**
front end set up 

cd frontend
pip install -r requirements.txt
streamlit run app.py


Backend setup 
cd backend
pip install -r requirements.txt
uvicorn main:app --reload

env variables 
GROQ_API_KEY=your_api_key_here
MODEL_NAME=openai/gpt-oss-120b
